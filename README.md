# Tensorflow Specialization on Coursera 

Programming assignments, high level learning ideas from all courses in the Coursera [TensorFlow specialization ](https://www.coursera.org/professional-certificates/tensorflow-in-practice) . Some of the synthetic data (CGI images) was generated by [Laurence Moroney](https://laurencemoroney.com/datasets.html)

## Credits
This repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the [TensorFlow specilization ](https://www.coursera.org/professional-certificates/tensorflow-in-practice), unless specified otherwise.

## Programming Projects
This page will describe in short about each project, the core learning and some tips.

| Project Name | Task | Training set size | Validation Set Size | Architechure | Epochs | Training Accuracy | validation Accuracy | Training time per epoch | Notes |
| ------------ | ---- | ----------------- | ------------------- | ------------ | ------ | ----------------- | ------------------- | ----- | ------ | 
| Fashion MNIST| MultiClass Classification (10) | 60K | 10K | DNN (512x512) | 50 | 97.6% | 89% | 4 sec | Nicely centered 28x28 images |  
| Fashion MNIST| MultiClass Classification (10) | 60K | 10K | CNN (2 layers + dense)| 50 | 99.6% | 90.81% | 5 sec | Nicely centered 28x28 images. |  
| Digit MNIST| MultiClass Classification (10) | 60K | 10K | DNN (512x512)| 50 | 99.8% | 98.1% | 4 sec | Nicely centered 28x28 images. |  
| Digit MNIST| MultiClass Classification (10) | 60K | 10K | CNN (2 layers + dense) | 50 | 99.99% | 99.31% | 5 sec | Nicely centered 28x28 images. |  
| Horse vs Human| Binary Classification (2) | 1K | 512 | CNN (5 layers + dense) | 50 | 100% | 84% | 8 sec | Centered 300x300 CGI images. |  
| Horse vs Human with data agumentation| Binary Classification (2) | 1K | 512 | CNN (5 layers + dense) | 100 | 95.4% | 63.28% | 8 sec | Data agumentation didn't helped as images validation set is similar to training set |
| Horse vs Human| Binary Classification (2) | 1K | 512 | Inception model (Transfer learning) | 10 | 99.7% | 99.6% | 12 sec | Transfer learning brings high accuracy! |  
| Cats Vs Dog | Binary Classification (2) | 2K | 1K | CNN (4 layers + dense) | 100 | 100% | 75% | 9 sec | Various size imags scraped from internet.(Kaggle) |
| Cats Vs Dog with data agumentation| Binary Classification (2) | 2K | 1K | CNN (4 layers + dense) | 100  | 84.2% | 79.6% | 16 sec | Data agumentation helped  |
| Cats Vs Dog | Binary Classification (2) | 2K | 1K | Inception Model (transfer learning) | 20 | 95.4% | 95.20% | 20 sec | Transfer learning brings high accuracy! |
| Cats Vs Dog | Binary Classification (2) | 22.5K | 2.5K | CNN (3 layers + dense) | 15 | 85% | 85% | 100 sec | Various size imags scraped from internet.(Kaggle) |
| Cats Vs Dog with data agumentation| Binary Classification (2) | 22.5K | 2.5K | CNN (3 layers + dense) | 15 | 73% | 70% | 300 sec | Data agumentation didn't help  |
| Sign language detection | Multiclass Classification (26) | 27.5K | 7K | CNN (2 layers + dense) | 20 | 87% | 93% | 30 sec |   |

### [Course 1: Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning](https://www.coursera.org/learn/introduction-tensorflow/home/welcome)
  This course mainly talk about the how to implement some of the classification task in tensorflow using deep neural network and CNN. Here is the week wise summary of the course.
  -  **Week 1:** Talks mainly about a very basic model (sort of hello world of tensorflow). It is illustrated by fitting a model in hourse price data (just few x(bedrooms),y (price) examples). 
  -  **Week 2:** Introduces to deep neural network (2 layer) as well as idea of callback to stop training when the desired accuracy is achieved.The labs uses training on fashion MNIST (10 labels) and digit MNIST dataset (10 labels). Both of these dataset have small (28x28) images centred around objects. This make achieving high accuracy feasible. With little effort on these task we can achieve >90% training accuracy. Digit MNIST is easier to train and even a 2 layer deep network can achieve 99% accuracy. Fashion MNIST (2 layerdeep ) is able to achieve ~90% accuracy.
  -  **Week 3:** Introduces to the idea of CNN which is then applied to fashion MNIST. Also by choosing the runtime environment to GPU (in Colab) we can observe 25X improvement in training speed. The genaral idea is to use few repetition of CONV --> MAX_POOL layers before feeding the output to dense layer.
  -  **Week 4:** Introduces the idea of ImageGenerator object which is really cool as it can read, label (based on directory name) and resize images on the runtime. The ImageGenerator object (training and validation) can then be passed to model.fit(). The programming assigment takes on real horse and human images to make a horseVshuman classifier. The project takes ~1000 images for the classification task.
  - High level ideas are capured in [Summary of Tensorflow course #1](https://github.com/dpant/TensorFlow/blob/main/Course_1_full_summary.ipynb) 
  - Individual weeks labs are captured at [course 1 labs](https://github.com/dpant/TensorFlow/tree/main/Course1)
  
### [Course 2: Convolutional Neural Networks in TensorFlow](https://www.coursera.org/learn/convolutional-neural-networks-tensorflow/home/welcome)
  This course mainly teach about the how to implement CNN's and associated tips. It dives deep into ImageDataGenerator() and its usefullness to create training and validation data ready to be fed into model.fit(). ImageDataGenerator() have lot of useful parameters to do data agumentation on the fly.Another powerful technique for getting good accuracy is transfer learning. Keras (under application modules) provide tons of models with their pre-trained weights. This is one of the best way to achieve high accuracy (Even if you have less data or limited compute resources). Finally with bit of modification in loss function we can do multi class classficiation in CNN's.
  -  **Week 1:** Talks about taking real world images and do a classification task. The data in this project is not very clean and images are not fixed size or centered around the object. There are multiple objects in some of the images. The main idea is to use ImageDataGenerator class to read the data, prepare its labels, resize it appropriately.
      -  [Project #1: Cats and dogs classificaiton using 3K images.](https://github.com/dpant/TensorFlow/blob/main/Course2/catVsDogs3KDataPoint%20%20-%20Notebook.ipynb)
          - This project shows how to do data preprocessing and generating labels using ImageDataGenerator objects. These objects can be training_generator and validation_generator which can be directly fed into the model.fit().This project is overfitting. Training accuracy is >98% while validation test is ~70%. Its a huge gap! and this is partly because we don't have sufficient data for this task (~2K traiing images) 
       - [Project #2 Cats vs Dogs Classification using 25K images](https://github.com/dpant/TensorFlow/blob/main/Course2/catVsDogs3KDataPoint%20%20-%20Notebook.ipynb). 
          - This project have 25K images in total. Generated training accuracy of 87% while test accuracy is 85%. So there is no overfitting. Since the dataset is large it is best to use GPU to train it. Took around 25 mins to train for 15 epochs in a GPU (Colab Pro plan). This again highlights the fact that having bigger dataset help in reducing the overfitting.
 
 -  **Week 2:** This week is all about data agumentation. Data agumentation can be really useful tool if one have limited data to do the training. For example if in cats and dogs classification we have 3K images, we will overfit the data as shown in project #1 about. Data agumentation can help in avoiding this. The most common data agumentation techniques available in tf.keras.preprocessing.image.ImageDataGenerator are, shear,zoom,rotation,horizontal_filp,vertical_flip, shifting(width_shift_range,height_shift_range). All of these transformation radomly choosen to be applied during training time for each epoch. Hence ImageDataGenerator() is not creating new images , instead it is modifying the existing images by doing these transformation (choosen randomly). Epoch to epoch accuracy/loss will vary more if we apply these transformations (due to to random transformation applied).
For the validation set the transformations are not applied. [Keras ImageDataGenerator](https://keras.io/preprocessing/image/)

     - [Project #3: Cats vs dogs classificaiton using 3K images + data agumentation.](https://github.com/dpant/TensorFlow/blob/main/Course2/CatsVsDogsWithDataAgumentation.ipynb)
        - Data agumentation helped a lot here in the original training we had 100% accuracy in training vs 75% in validation set which is clearly a sign of overfitting. With data agumentation we have 84% accuracy in training and 79% in validation set. Thus the gap in accuracy is narrowed (overfitting is reduced)
        
     - [Project #4: Horse vs human classificaiton using 1K images + data agumentation.](https://github.com/dpant/TensorFlow/blob/main/Course2/HorseVsHumanDataAgumentation.ipynb) 
          - Notice that Data Agumentation does not always help in reducing the overfitting (gap between accuracy of training,validation). For example the horse vs human applying data agumentation did not yeild good result.So data agumentation is not a substitute of good quality diverse image data. In this particular project both train and validation set have similar type of data (images) so doing data agumentation did not helped (In fact added random noise in training and validation accuracy). At the end of 100 epochs we have 95% training accuracy vs 63% validation accuracy.Validating accuracy fluctuate widely here as we are chainging the training set in every epoch by adding random transformations.
      
 -  **Week 3:** Transfer learning.  Transfer learning is a very good way to bootstrap your model. Use a pretrained model (with lots of data + weeks of GPU training), add few layers at the end of model, retrain with your own data and reap the benefit of transfer learning. Often the benefits of using transfer learnign is > 10% in training and validatino sets.Keras  provide a good API for this. Keras have many pretrained network (with their weights). [Keras pretrained models](https://www.tensorflow.org/api_docs/python/tf/keras/applications). Also you can use the weights= parameters to specify picking up pre-trained weights or use a file to supply weights.        
     - [Project #5: Cats vs dogs classificaiton using 3K images using transfer learning.](https://github.com/dpant/TensorFlow/blob/main/Course2/TransferLearningInceptionModelCatsVsDogs3k.ipynb)
        - Transfer learning help immensely in this dataset. The accuracy bumped up to 95% (for both training and validation). This is > 10% improvement. Transfer learning is very widely used. 
     - [Project #6: Horse vs Human classificaiton using 1K images using transfer learning.](https://github.com/dpant/TensorFlow/blob/main/Course2/TransferLearningInceptionModelHorsesvsHumans1K.ipynb)
        - Transfer learning help immensely in this dataset. The accuracy bumped up to 99% (for both training and validation). This is > 16% improvement. Transfer learning is very widely used and gives good results.
     
-  **Week 4:** MultiClass Classification. Multiclass classifiction is very similar to binary classification. We just have to adjust the loss function and output layer of the neural network to be softmax.
     - [Project #7: Kaggle Sign language detection.](https://github.com/dpant/TensorFlow/blob/main/Course2/KaggleSignDetectionMultiClassClassification.ipynb)
        - This project detect the sign language. It have 27K images for training and additional 7k for validation. The output labels are 26 (corresponding to A-Z). Using a simple 2 Conv layer --> Dense layer model we were able to achive validation accuracy of 93%. Since the dataset is large and mostly have well centered single images it is not hard to get a good accuarcy in validation set. [Kaggle link for data and more about dataset](https://www.kaggle.com/datamunge/sign-language-mnist)

### [Course 3: Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome)
  NLP course mainly teaches about the how to implement NLP application and associated tips. Tokenization is one of the first step in NLP. While writing these application it is important that NLP does not just cater English as a language. There are lot of techniques in NLP which are developed due to work on diverse set of language like handling morphologically heavy or Agglutination heavy language. Subword tokenization which makes bit less sense for English is a absolute must for [kichwa](https://en.wikipedia.org/wiki/Agglutination#/media/File:Riksirishkakunapalla.png). Tokenization should be in such a way to minmize OOV (out of vocab) words in validation set. 
  One things is very common to have overfitting in NLP datasets. This is due to 1) vocab is extracted using training data and there can be unseen tokens in validation (test) datasets. 2) Not using regularization (droupts). So look out of increase (gap) in validation loss during the training cycle. 
  
  -  **Week 1:** Talks mainly about encoding the text data using tensorflow Tokenizer() objects and its helper functions like fit_to_texts (for creating the dictionary for encoding) and text_to_sequences for converting the sentences to numeric list of encoded numbers. Also the pad_sequences() object is very useful to make all sequences of same size (either by prefix padding or post padding and truncating). The final data should be of same size examples to be fed into neural network so these steps are necessary. At the end of pad_sequence() we get all our data in one big matrix with (# of sentences, max len) Encoding the strings to number is the first step to prepare it to feed to neural network (embedding layer of dense layer). 

      -  [Project #1: Tokenization of BBC news dataset.](https://github.com/dpant/TensorFlow/blob/main/Course3-NLP/BBCdataSetTokenization.ipynb)
          - This project remove stopwords, tokenize the sentenses ,assign each word a unique code, pad sentences with 0, truncate larger then threshold sentences. This is done using Kera's API namely Tonkenizer() and pad_sequences()
  
  -  **Week 2:** To attach meaning (sense of closeness for related word) one need to find the embedding (usually the first layer of NN) of each word. Embedding can either be learned as a part of NN or we can use pre-existing embedding (sort of transfer learning). So the  NLP training flow looks like:
        - Tokenize --> ConvertToIntegerCode --> Padding (same size) --> Embedding (generate encoding in vector space for closeness) --> NeuralNetwork   
        - Tensor flow provide lot of pre packaged and pre-processed datasets for experimentation and learning https://www.tensorflow.org/datasets/catalog/overview
        - https://projector.tensorflow.org/ is a resource to view the embedding if you have dumped the embedding files.  
       -  [Project #2: Moview review sentiment analysis (IMDB).](https://github.com/dpant/TensorFlow/blob/main/Course3-NLP/imdbReviewClassificationEmbedding.ipynb)
          - Movie sentiment classification. vocab_size = 10000 embedding_dim = 16 , traiing example = 25K , validation example 25K. Training accuracy 100% , Validation accuracy 82% (Overfitting!)

  -  **Week 3:** Word embedding is *good in capturing single word meaning* but it does not capture the *context or meaning of word due to its surrondings*. To capture context (semantics) we either need to use RNN,LSTM,GRU cells. RNN is good to capture nearby context (short range conxtexts). RNN did suffer with vanishing gradient problems also. LSTM (or GRU) is a fix for avoiding vanishing gradient and capturing long range dependencies (context). Long range dependencies are very common in natural languages. These RNN/GRU/LSTM layer can be stacked too generating deep RNN or Deep LSTM's
      -  Look out of overfitting in NLP training. Use droputs, or pretrained embedding to reduce overfitting.
      -  If your training accuracy is low, this implies bad preprocessing (use nltk for stemming, stop word removal etc). You can also use pretrained model like transformers  (transer learning) or make your model more complex (more layers etc) 
      -  Also in some cases you can use [CONV1D layer instead of LSTM](https://datascience.stackexchange.com/questions/78030/multivariate-time-series-analysis-when-is-a-cnn-vs-lstm-appropriate). 
    - [Project #3: Moview review sentiment analysis (IMDB).](https://github.com/dpant/TensorFlow/blob/main/Course3-NLP/imdbReviewClassificationEmbedding.ipynb)
          - Movie sentiment classification. vocab_size = 10000 embedding_dim = 16 , traiing example = 25K , validation example 25K. Training accuracy 100% , Validation accuracy 82% (Overfitting!)

          
     
